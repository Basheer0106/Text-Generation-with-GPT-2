# Text-Generation-with-GPT-2
This project demonstrates text generation using GPT-2, a transformer-based language model that generates coherent and context-aware text based on a given input prompt.
### Overview
This project focuses on generating human-like text using GPT-2, a transformer-based language model developed by OpenAI.

### Features
- Prompt-based text generation
- Uses pre-trained GPT-2 model
- Simple and clear implementation
- Customizable generation parameters

### Tools & Technologies
- Python
- GPT-2
- Hugging Face Transformers
- Google Colab

### Execution
The project was developed and executed in Google Colab.
